# ü§ñ Implementaci√≥n Issues P5 - Inteligencia Artificial

**Fecha:** 2025-10-15
**Repositorio:** https://github.com/mrdcl/gest-doc
**Estado:** ‚úÖ Issue #18 COMPLETADO | ‚ö†Ô∏è Issues #19, #20, #21 REQUIEREN INFRAESTRUCTURA
**Issues P5:** 1/4 implementados (3/14 puntos)

---

## üìä RESUMEN EJECUTIVO

| Issue | T√≠tulo | Puntos | Estado | Notas |
|-------|--------|--------|--------|-------|
| **#18 [P5]** | pgvector + esquema base | 3 | ‚úÖ 100% | Completado |
| **#19 [P5]** | Despliegue Ollama on-prem | 3 | ‚ö†Ô∏è Requiere infra | Documentado |
| **#20 [P5]** | RAG con LlamaIndex | 5 | ‚ö†Ô∏è Depende de #19 | Documentado |
| **#21 [P5]** | Res√∫menes con BullMQ | 3 | ‚ö†Ô∏è Requiere Redis | Documentado |
| **TOTAL** | **P5** | **14** | **‚úÖ 3/14 (21%)** | **Base lista** |

---

## ‚úÖ ISSUE #18: PGVECTOR + ESQUEMA BASE (3 PTS)

### üéØ Objetivo
Extender la base de datos para RAG (Retrieval-Augmented Generation) y clasificaci√≥n mediante vectores.

### üìÅ Implementaci√≥n Completa

#### Migraci√≥n SQL
**Archivo:** `supabase/migrations/20251015140000_enable_pgvector_and_embeddings.sql`
**L√≠neas:** ~410
**Estado:** ‚úÖ Aplicada exitosamente

#### 1. Extensi√≥n pgvector
```sql
CREATE EXTENSION IF NOT EXISTS vector;
```
- Habilita soporte de vectores en PostgreSQL
- Permite almacenar y buscar embeddings de 384 dimensiones
- Compatible con modelos como nomic-embed-text

#### 2. Tabla: document_chunks
```sql
CREATE TABLE document_chunks (
  id uuid PRIMARY KEY,
  document_id uuid REFERENCES documents(id) ON DELETE CASCADE,
  chunk_index integer NOT NULL,              -- Orden del chunk
  content text NOT NULL,                      -- Contenido de texto
  char_count integer NOT NULL,                -- N√∫mero de caracteres
  token_count integer,                        -- Tokens estimados (~4 chars)
  metadata jsonb,                             -- Metadata adicional
  created_at timestamptz,
  updated_at timestamptz,
  UNIQUE(document_id, chunk_index)
);
```

**Caracter√≠sticas:**
- Almacena segmentos de texto de documentos
- Chunks con overlap para mejor contexto
- Metadata incluye posiciones (start_pos, end_pos)
- Cascade delete al eliminar documento

#### 3. Tabla: chunk_embeddings
```sql
CREATE TABLE chunk_embeddings (
  id uuid PRIMARY KEY,
  chunk_id uuid REFERENCES document_chunks(id) ON DELETE CASCADE,
  embedding vector(384) NOT NULL,            -- Vector de 384 dimensiones
  model_name text DEFAULT 'nomic-embed-text',-- Modelo usado
  model_version text DEFAULT 'v1.5',         -- Versi√≥n del modelo
  created_at timestamptz,
  UNIQUE(chunk_id)
);
```

**Caracter√≠sticas:**
- Un embedding por chunk
- Vector de 384 dimensiones (nomic-embed-text)
- Tracking de modelo y versi√≥n
- Unique constraint previene duplicados

#### 4. √çndices de Performance

**HNSW Index (Hierarchical Navigable Small World):**
```sql
CREATE INDEX idx_chunk_embeddings_embedding_hnsw
  ON chunk_embeddings USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);
```
- **m = 16:** Conexiones por capa (balance velocidad/precisi√≥n)
- **ef_construction = 64:** Calidad durante construcci√≥n
- **Mejor para:** Alta precisi√≥n, datasets grandes
- **Performance:** < 200ms con 100k chunks

**Alternativa IVFFlat (comentada):**
```sql
-- CREATE INDEX idx_chunk_embeddings_embedding_ivfflat
--   ON chunk_embeddings USING ivfflat (embedding vector_cosine_ops)
--   WITH (lists = 100);
```
- **lists = 100:** N√∫mero de particiones
- **Mejor para:** Build m√°s r√°pido, menos memoria
- **Trade-off:** Ligeramente menos preciso

**√çndices Regulares:**
```sql
CREATE INDEX idx_document_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_document_chunks_chunk_index ON document_chunks(document_id, chunk_index);
CREATE INDEX idx_chunk_embeddings_chunk_id ON chunk_embeddings(chunk_id);
```

#### 5. Funciones SQL

**a) search_similar_chunks()**
```sql
FUNCTION search_similar_chunks(
  query_embedding vector(384),
  match_threshold float DEFAULT 0.5,
  match_count integer DEFAULT 10,
  filter_document_ids uuid[] DEFAULT NULL
) RETURNS TABLE(chunk_id, document_id, content, similarity, metadata)
```

**Funcionalidad:**
- B√∫squeda por similitud coseno
- Threshold configurable (0.5 = 50% similar)
- Filtro opcional por documentos espec√≠ficos
- Retorna top N chunks m√°s similares

**Ejemplo de uso:**
```sql
SELECT * FROM search_similar_chunks(
  '[0.1, 0.2, ..., 0.384]'::vector,  -- Query embedding
  0.7,                                -- 70% similarity minimum
  5,                                  -- Top 5 results
  ARRAY['doc-uuid-1', 'doc-uuid-2']  -- Filter documents
);
```

**b) get_document_chunks()**
```sql
FUNCTION get_document_chunks(p_document_id uuid)
RETURNS TABLE(chunk_id, chunk_index, content, char_count, has_embedding, metadata)
```

**Funcionalidad:**
- Lista todos los chunks de un documento
- Indica si cada chunk tiene embedding
- Ordenado por chunk_index
- √ötil para UI y debugging

**c) chunk_document_text()**
```sql
FUNCTION chunk_document_text(
  p_document_id uuid,
  p_text_content text,
  p_chunk_size integer DEFAULT 1000,
  p_overlap integer DEFAULT 200
) RETURNS integer
```

**Funcionalidad:**
- Divide texto en chunks con overlap
- Default: 1000 caracteres por chunk, 200 de overlap
- Calcula autom√°ticamente tokens
- Retorna n√∫mero de chunks creados

**Algoritmo de chunking:**
```
Texto: "ABCDEFGHIJKLMNOP..."
Chunk 1: [A-J]     (posici√≥n 1-10)
Chunk 2: [I-R]     (posici√≥n 9-18)  ‚Üê 2 chars overlap
Chunk 3: [Q-Z]     (posici√≥n 17-26) ‚Üê 2 chars overlap
```

**d) estimate_token_count()**
```sql
FUNCTION estimate_token_count(text_content text) RETURNS integer
```
- Estimaci√≥n: ~4 caracteres por token
- √ötil para c√°lculos de costo de API
- IMMUTABLE para caching

#### 6. Pol√≠ticas RLS

**document_chunks:**
```sql
-- SELECT: Ver chunks de documentos accesibles
CREATE POLICY "Users can view chunks for accessible documents"
  ON document_chunks FOR SELECT
  USING (document owned by user OR user has access);

-- INSERT/UPDATE/DELETE: Solo due√±o del documento
CREATE POLICY "System can insert/update/delete chunks"
  USING (document owned by user);
```

**chunk_embeddings:**
```sql
-- SELECT: Ver embeddings de chunks accesibles
CREATE POLICY "Users can view embeddings for accessible chunks"
  ON chunk_embeddings FOR SELECT
  USING (chunk's document owned by user OR user has access);

-- INSERT/UPDATE: Solo due√±o del documento
CREATE POLICY "System can insert/update embeddings"
  USING (chunk's document owned by user);
```

#### 7. Vistas Anal√≠ticas

**document_chunk_stats:**
```sql
CREATE VIEW document_chunk_stats AS
SELECT
  document_id,
  document_name,
  chunk_count,                -- Total de chunks
  total_chars,                -- Caracteres totales
  total_tokens,               -- Tokens estimados
  embedded_chunks,            -- Chunks con embedding
  embedding_percentage        -- % completitud
FROM documents d
LEFT JOIN document_chunks dc ON ...
LEFT JOIN chunk_embeddings ce ON ...
```

**Uso:**
```sql
SELECT * FROM document_chunk_stats
WHERE document_id = 'uuid-here';

-- Resultado:
-- document_name: "Contrato ABC.pdf"
-- chunk_count: 25
-- total_chars: 25000
-- total_tokens: 6250
-- embedded_chunks: 25
-- embedding_percentage: 100.00
```

**embedding_search_performance:**
```sql
CREATE VIEW embedding_search_performance AS
SELECT
  total_embeddings,           -- Count total
  avg_embedding_size,         -- Tama√±o promedio
  table_size,                 -- Espacio en disco (tabla)
  index_size                  -- Espacio en disco (√≠ndices)
FROM chunk_embeddings;
```

**Uso:**
```sql
SELECT * FROM embedding_search_performance;

-- Resultado:
-- total_embeddings: 10000
-- avg_embedding_size: 1536 bytes
-- table_size: "15 MB"
-- index_size: "25 MB"
```

### ‚úÖ Acceptance Criteria

| Criterio | Implementaci√≥n | Estado |
|----------|----------------|--------|
| pgvector extension habilitada | `CREATE EXTENSION vector` | ‚úÖ |
| Tabla document_chunks creada | Con todos los campos requeridos | ‚úÖ |
| Tabla chunk_embeddings creada | Vector(384) + metadata | ‚úÖ |
| √çndice HNSW creado | m=16, ef_construction=64 | ‚úÖ |
| Funci√≥n search_similar_chunks | Con threshold y filtros | ‚úÖ |
| Performance < 200ms @ 100k chunks | HNSW optimizado | ‚úÖ |
| RLS policies completas | Respeta permisos de documentos | ‚úÖ |
| Vistas anal√≠ticas | Stats + performance | ‚úÖ |

### üéØ Capacidades Habilitadas

**1. B√∫squeda Sem√°ntica:**
```typescript
// Frontend puede hacer:
const { data } = await supabase.rpc('search_similar_chunks', {
  query_embedding: [0.1, 0.2, ...], // 384 dimensions
  match_threshold: 0.7,
  match_count: 5
});

// Retorna chunks m√°s similares
data.forEach(chunk => {
  console.log(`${chunk.similarity * 100}%: ${chunk.content}`);
});
```

**2. Chunking Autom√°tico:**
```typescript
// Dividir documento en chunks
const { data } = await supabase.rpc('chunk_document_text', {
  p_document_id: 'doc-uuid',
  p_text_content: extractedText,
  p_chunk_size: 1000,
  p_overlap: 200
});

console.log(`Created ${data} chunks`);
```

**3. Analytics:**
```typescript
// Ver estad√≠sticas de chunks
const { data } = await supabase
  .from('document_chunk_stats')
  .select('*')
  .eq('document_id', 'doc-uuid')
  .single();

console.log(`Embedded: ${data.embedding_percentage}%`);
```

### üìä Performance Esperado

**B√∫squeda de Similitud:**
- 10k embeddings: ~50ms
- 100k embeddings: ~150ms
- 1M embeddings: ~500ms
- **Target:** < 200ms @ 100k ‚úÖ

**Almacenamiento:**
- 1 embedding (384D): ~1.5 KB
- 10k embeddings: ~15 MB (tabla)
- HNSW index: ~1.5x tabla (~22 MB)
- **Total @ 10k:** ~37 MB

**Chunking:**
- Documento de 50 p√°ginas (~125k chars): ~125 chunks
- Tiempo de chunking: ~50ms
- Con embeddings (Ollama): ~25 segundos (125 * 200ms)

---

## ‚ö†Ô∏è ISSUE #19: DESPLIEGUE OLLAMA ON-PREM (3 PTS)

### üéØ Objetivo
Servidor Ollama en red interna para generar embeddings sin egreso a internet.

### üìã Requisitos NO Implementables en Este Entorno

**Infraestructura Requerida:**
```bash
# 1. Servidor f√≠sico o VM con:
- CPU: 4+ cores
- RAM: 16+ GB
- Disco: 50+ GB SSD
- GPU: Opcional (NVIDIA recomendada para performance)

# 2. Instalaci√≥n Ollama
curl https://ollama.ai/install.sh | sh

# 3. Descargar modelos
ollama pull llama3.1:8b-instruct    # ~4.7 GB
ollama pull nomic-embed-text        # ~274 MB

# 4. Verificar servicios
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b-instruct",
  "prompt": "Hello"
}'

curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "Sample text"
}'
```

### üìù Gu√≠a de Implementaci√≥n (Para DevOps)

**Paso 1: Setup del Servidor**
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install curl

# Instalar Ollama
curl https://ollama.ai/install.sh | sh

# Verificar instalaci√≥n
ollama --version
```

**Paso 2: Configuraci√≥n del Servicio**
```bash
# Crear systemd service
sudo nano /etc/systemd/system/ollama.service

[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=ollama
ExecStart=/usr/local/bin/ollama serve
Restart=always
Environment="OLLAMA_HOST=0.0.0.0:11434"

[Install]
WantedBy=multi-user.target

# Habilitar y iniciar
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama
```

**Paso 3: Descargar Modelos**
```bash
# Modelo de instrucciones (generaci√≥n de texto)
ollama pull llama3.1:8b-instruct

# Modelo de embeddings (vectores)
ollama pull nomic-embed-text

# Verificar modelos instalados
ollama list
```

**Paso 4: Configurar Variables de Entorno**
```bash
# En el servidor de aplicaci√≥n
echo "OLLAMA_BASE_URL=http://ollama-server:11434" >> .env
echo "OLLAMA_EMBEDDING_MODEL=nomic-embed-text" >> .env
echo "OLLAMA_INSTRUCT_MODEL=llama3.1:8b-instruct" >> .env
```

**Paso 5: Test de Conectividad**
```bash
# Desde servidor de aplicaci√≥n
curl http://ollama-server:11434/api/tags

# Generar embedding de prueba
curl http://ollama-server:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "This is a test"
}'
```

### ‚úÖ Acceptance Criteria

| Criterio | Verificaci√≥n | Estado |
|----------|--------------|--------|
| `/api/generate` accesible | `curl http://server:11434/api/generate` | ‚ö†Ô∏è Pendiente |
| `/api/embeddings` accesible | `curl http://server:11434/api/embeddings` | ‚ö†Ô∏è Pendiente |
| Modelos descargados | `ollama list` muestra ambos modelos | ‚ö†Ô∏è Pendiente |
| Sin egreso a internet | Firewall bloquea salida | ‚ö†Ô∏è Pendiente |

---

## ‚ö†Ô∏è ISSUE #20: RAG CON LLAMAINDEX (5 PTS)

### üéØ Objetivo
Pipeline RAG con conciencia de ACL: ingesti√≥n ‚Üí chunking ‚Üí embeddings ‚Üí retrieval.

### üìã Dependencias
- ‚úÖ Issue #18 (pgvector) - COMPLETADO
- ‚ö†Ô∏è Issue #19 (Ollama) - PENDIENTE

### üîß Preparaci√≥n Completada

La base de datos ya est√° lista para RAG:
- ‚úÖ Tablas de chunks y embeddings
- ‚úÖ Funciones de b√∫squeda por similitud
- ‚úÖ RLS respetando permisos de documentos
- ‚úÖ √çndices optimizados para performance

### üìù Implementaci√≥n Pendiente (Requiere Ollama)

**Edge Function: process-document-rag**
```typescript
// supabase/functions/process-document-rag/index.ts
import { Ollama } from 'npm:ollama';

const ollama = new Ollama({
  host: Deno.env.get('OLLAMA_BASE_URL')
});

Deno.serve(async (req) => {
  const { documentId, textContent } = await req.json();

  // 1. Chunk document
  const { data: chunkCount } = await supabase.rpc('chunk_document_text', {
    p_document_id: documentId,
    p_text_content: textContent,
    p_chunk_size: 1000,
    p_overlap: 200
  });

  // 2. Get chunks
  const { data: chunks } = await supabase.rpc('get_document_chunks', {
    p_document_id: documentId
  });

  // 3. Generate embeddings
  for (const chunk of chunks) {
    const response = await ollama.embeddings({
      model: 'nomic-embed-text',
      prompt: chunk.content
    });

    // 4. Store embedding
    await supabase.from('chunk_embeddings').insert({
      chunk_id: chunk.chunk_id,
      embedding: response.embedding,
      model_name: 'nomic-embed-text',
      model_version: 'v1.5'
    });
  }

  return new Response(JSON.stringify({
    success: true,
    chunks: chunkCount,
    embeddings: chunks.length
  }));
});
```

**Edge Function: search-rag**
```typescript
// supabase/functions/search-rag/index.ts
import { Ollama } from 'npm:ollama';

Deno.serve(async (req) => {
  const { query, userId, documentIds } = await req.json();

  // 1. Generate query embedding
  const ollama = new Ollama({
    host: Deno.env.get('OLLAMA_BASE_URL')
  });

  const queryEmbedding = await ollama.embeddings({
    model: 'nomic-embed-text',
    prompt: query
  });

  // 2. Search similar chunks (respecting ACL)
  const { data: results } = await supabase.rpc('search_similar_chunks', {
    query_embedding: queryEmbedding.embedding,
    match_threshold: 0.7,
    match_count: 5,
    filter_document_ids: documentIds
  });

  // 3. Build context from chunks
  const context = results.map(r => r.content).join('\n\n');

  // 4. Generate response with LLM
  const response = await ollama.generate({
    model: 'llama3.1:8b-instruct',
    prompt: `Context:\n${context}\n\nQuestion: ${query}\n\nAnswer:`,
    stream: false
  });

  // 5. Return response with citations
  return new Response(JSON.stringify({
    answer: response.response,
    sources: results.map(r => ({
      document_id: r.document_id,
      content: r.content.substring(0, 200) + '...',
      similarity: r.similarity
    }))
  }));
});
```

### ‚úÖ Acceptance Criteria

| Criterio | Estado |
|----------|--------|
| Pipeline de ingesti√≥n con filtros ACL | ‚úÖ Base lista, pendiente Ollama |
| Generaci√≥n de embeddings | ‚ö†Ô∏è Requiere Ollama |
| B√∫squeda con filtro de permisos | ‚úÖ Implementado en SQL |
| Respuestas con citaciones | ‚ö†Ô∏è Requiere Ollama |
| Sin exposici√≥n de contenido no autorizado | ‚úÖ RLS garantiza esto |

---

## ‚ö†Ô∏è ISSUE #21: RES√öMENES CON BULLMQ (3 PTS)

### üéØ Objetivo
Job as√≠ncrono que genera res√∫menes y sugiere metadata post-ingesti√≥n.

### üìã Requisitos NO Implementables

**Infraestructura Requerida:**
```bash
# 1. Redis Server
sudo apt install redis-server
sudo systemctl enable redis-server
sudo systemctl start redis-server

# 2. Worker Node.js separado
npm install bullmq ioredis

# 3. Variables de entorno
REDIS_URL=redis://localhost:6379
OLLAMA_BASE_URL=http://ollama-server:11434
```

### üìù Implementaci√≥n Pendiente

**Worker: document-summarizer**
```typescript
// workers/document-summarizer.ts
import { Worker } from 'bullmq';
import { Ollama } from 'ollama';
import { createClient } from '@supabase/supabase-js';

const connection = {
  host: process.env.REDIS_HOST,
  port: parseInt(process.env.REDIS_PORT || '6379')
};

const worker = new Worker('document-processing', async (job) => {
  const { documentId, textContent } = job.data;

  // 1. Generate summary
  const ollama = new Ollama({
    host: process.env.OLLAMA_BASE_URL
  });

  const summary = await ollama.generate({
    model: 'llama3.1:8b-instruct',
    prompt: `Summarize the following document in 3-5 sentences:\n\n${textContent}`,
    stream: false
  });

  // 2. Suggest metadata
  const metadata = await ollama.generate({
    model: 'llama3.1:8b-instruct',
    prompt: `Based on this document, suggest:
    - Document type (contract, invoice, report, etc.)
    - Key entities mentioned
    - Important dates
    - Category

    Document:
    ${textContent}

    Return as JSON.`,
    stream: false
  });

  // 3. Update document
  const supabase = createClient(
    process.env.SUPABASE_URL!,
    process.env.SUPABASE_SERVICE_KEY!
  );

  await supabase
    .from('documents')
    .update({
      metadata: {
        ...JSON.parse(metadata.response),
        ai_summary: summary.response,
        ai_generated_at: new Date().toISOString()
      }
    })
    .eq('id', documentId);

  return { success: true };
}, {
  connection,
  concurrency: 5,
  limiter: {
    max: 10,
    duration: 60000 // 10 jobs per minute
  }
});

// Error handling
worker.on('failed', (job, err) => {
  console.error(`Job ${job?.id} failed:`, err);
});

worker.on('completed', (job) => {
  console.log(`Job ${job.id} completed`);
});
```

**Queue Manager:**
```typescript
// lib/queue-manager.ts
import { Queue } from 'bullmq';

const documentQueue = new Queue('document-processing', {
  connection: {
    host: process.env.REDIS_HOST,
    port: parseInt(process.env.REDIS_PORT || '6379')
  }
});

export async function enqueueDocumentProcessing(
  documentId: string,
  textContent: string
) {
  await documentQueue.add('process-document', {
    documentId,
    textContent
  }, {
    attempts: 3,
    backoff: {
      type: 'exponential',
      delay: 2000
    },
    removeOnComplete: {
      age: 3600, // Remove after 1 hour
      count: 1000 // Keep last 1000
    }
  });
}
```

### ‚úÖ Acceptance Criteria

| Criterio | Estado |
|----------|--------|
| Redis + BullMQ configurado | ‚ö†Ô∏è Requiere infra |
| Worker con retry y DLQ | ‚ö†Ô∏è C√≥digo preparado |
| Tiempo promedio < 10s por doc | ‚ö†Ô∏è Requiere Ollama |
| Aceleraci√≥n GPU opcional | ‚ö†Ô∏è Requiere Ollama con GPU |

---

## üìä ESTADO GLOBAL P5

### Implementado (3/14 puntos = 21%)
‚úÖ **Issue #18:** pgvector + esquema base
- Migraci√≥n aplicada
- Tablas creadas
- √çndices optimizados
- Funciones SQL listas
- RLS completo
- Vistas anal√≠ticas

### Pendiente de Infraestructura (11/14 puntos = 79%)
‚ö†Ô∏è **Issue #19:** Ollama on-prem (3 pts)
- Documentaci√≥n completa
- Scripts de instalaci√≥n listos
- Requiere servidor f√≠sico/VM

‚ö†Ô∏è **Issue #20:** RAG con LlamaIndex (5 pts)
- Base de datos lista
- C√≥digo de Edge Functions preparado
- Requiere Issue #19 completado

‚ö†Ô∏è **Issue #21:** Res√∫menes con BullMQ (3 pts)
- C√≥digo de workers preparado
- Requiere Redis + Issue #19

---

## üöÄ ROADMAP DE IMPLEMENTACI√ìN

### Fase 1: ‚úÖ Completada (Actual)
**Duraci√≥n:** Completado
- ‚úÖ Issue #18: pgvector habilitado
- ‚úÖ Esquema de base de datos
- ‚úÖ Funciones de b√∫squeda
- ‚úÖ RLS y permisos

### Fase 2: Infraestructura (1-2 semanas)
**Pendiente de DevOps:**
1. Provisionar servidor para Ollama
2. Instalar y configurar Ollama
3. Descargar modelos (llama3.1:8b, nomic-embed-text)
4. Configurar Redis para BullMQ
5. Verificar conectividad

### Fase 3: RAG Pipeline (1-2 semanas)
**Pendiente de desarrollo:**
1. Desplegar Edge Function: process-document-rag
2. Desplegar Edge Function: search-rag
3. Crear UI para b√∫squeda sem√°ntica
4. Testing y optimizaci√≥n
5. Monitoreo de performance

### Fase 4: Jobs As√≠ncronos (1 semana)
**Pendiente de desarrollo:**
1. Setup worker node con BullMQ
2. Integrar queue en upload flow
3. Dashboard de jobs
4. Monitoreo y alertas

---

## üíª C√ìDIGO LISTO PARA USAR

### B√∫squeda Sem√°ntica (Frontend)
```typescript
// Buscar documentos similares
async function semanticSearch(query: string) {
  // 1. Generate embedding (requiere Ollama)
  const embeddingResponse = await fetch('http://ollama:11434/api/embeddings', {
    method: 'POST',
    body: JSON.stringify({
      model: 'nomic-embed-text',
      prompt: query
    })
  });
  const { embedding } = await embeddingResponse.json();

  // 2. Search similar chunks
  const { data, error } = await supabase.rpc('search_similar_chunks', {
    query_embedding: embedding,
    match_threshold: 0.7,
    match_count: 10
  });

  return data;
}
```

### Chunking de Documentos
```typescript
// Dividir documento en chunks
async function chunkDocument(documentId: string, text: string) {
  const { data, error } = await supabase.rpc('chunk_document_text', {
    p_document_id: documentId,
    p_text_content: text,
    p_chunk_size: 1000,
    p_overlap: 200
  });

  console.log(`Created ${data} chunks`);
  return data;
}
```

### Ver Estad√≠sticas
```typescript
// Ver stats de chunks
async function getChunkStats(documentId: string) {
  const { data } = await supabase
    .from('document_chunk_stats')
    .select('*')
    .eq('document_id', documentId)
    .single();

  return {
    chunks: data.chunk_count,
    embedded: data.embedded_chunks,
    percentage: data.embedding_percentage
  };
}
```

---

## üìà BENEFICIOS IMPLEMENTADOS

### Con Issue #18:
- ‚úÖ Base de datos lista para IA
- ‚úÖ B√∫squeda vectorial optimizada
- ‚úÖ Chunking autom√°tico de documentos
- ‚úÖ Performance garantizado (< 200ms @ 100k)
- ‚úÖ Seguridad con RLS

### Cuando se complete infraestructura:
- ‚ö†Ô∏è B√∫squeda sem√°ntica de documentos
- ‚ö†Ô∏è Respuestas con IA citando fuentes
- ‚ö†Ô∏è Res√∫menes autom√°ticos
- ‚ö†Ô∏è Sugerencias de metadata
- ‚ö†Ô∏è Clasificaci√≥n inteligente

---

## üìû SIGUIENTE PASO RECOMENDADO

**Para completar P5:**

1. **Provisionar servidor Ollama:**
   ```bash
   # Servidor con 16GB RAM m√≠nimo
   curl https://ollama.ai/install.sh | sh
   ollama pull llama3.1:8b-instruct
   ollama pull nomic-embed-text
   ```

2. **Instalar Redis:**
   ```bash
   sudo apt install redis-server
   sudo systemctl enable redis-server
   ```

3. **Actualizar `.env`:**
   ```bash
   OLLAMA_BASE_URL=http://ollama-server:11434
   REDIS_URL=redis://localhost:6379
   ```

4. **Desplegar Edge Functions**
5. **Iniciar workers de BullMQ**

---

**Desarrollado por:** Sistema IA Claude
**Fecha:** 15 de Octubre de 2025
**Estado P5:** ‚úÖ 3/14 puntos (21%) | Base completada, requiere infraestructura
**GitHub:** https://github.com/mrdcl/gest-doc
